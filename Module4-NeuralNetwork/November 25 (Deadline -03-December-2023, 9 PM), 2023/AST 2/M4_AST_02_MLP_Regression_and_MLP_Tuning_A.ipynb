{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HklD4MlOgCTy"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 2: MLP Regression and MLP Tuning"]},{"cell_type":"markdown","metadata":{"id":"doENGez0k03C"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"HbFv4sGHk7VW"},"source":["At the end of the experiment, you will be able to\n","\n","- understand the concept of MLPs for regression\n","- know the hyperparameters of neural network and their tuning\n","- understand batch normalization using Keras\n","- understand the concept of optimizers\n","- understand the time-based learning rate method through an example\n","- understand the different regularization methods to avoid the overfitting of neural networks"]},{"cell_type":"markdown","metadata":{"id":"w-9AxpS_WMr9"},"source":["### Introduction to Regression MLPs"]},{"cell_type":"markdown","metadata":{"id":"btCQw9drWTH_"},"source":["First, MLPs can be used for regression tasks. If we want to predict a single value (e.g., the  price  of  a  house  given  many  of  its  features), then  we  just  need  a  single  output neuron:  its  output  is  the  predicted  value.  For  multivariate  regression  (i.e.,  to  predict multiple  values  at  once),  we  need  one  output  neuron  per  output  dimension.  For example, to locate the center of an object on an image, we need to predict 2D coordinates,  so  we  need  two  output  neurons.  If  we  also  want  to  place  a  bounding  box around the object, then we need two more numbers: the width and the height of the object. So we end up with 4 output neurons.\n","\n","In general, when building an MLP for regression, we do not want to use any activation  function  for  the  output  neurons,  so  they  are  free  to  output  any  range  of  values. However,  if  we  want  to  guarantee  that  the  output  will  always  be  positive,  then  we can use the ReLU activation function or the softplus activation function in the output layer.  Finally,  if  we  want  to  guarantee  that  the  predictions  will  fall  within  a  given range of values, then we can use the logistic function or the hyperbolic tangent and scale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for the hyperbolic tangent.\n","\n","The loss function to use during training is typically the mean squared error, but if we have  a  lot  of  outliers  in  the  training  set,  we  may  prefer  to  use  the  mean  absolute error  instead.  Alternatively,  we  can  use  the  Huber  loss,  which  is  a  combination  of both.\n","\n","**Note:** The Huber loss is quadratic when the error is smaller than a threshold $δ$ (typically 1), but linear when the error is larger than $δ$. This makes it less sensitive to outliers than the mean squared error, and\n","it  is  often  more  precise  and  converges  faster  than  the  mean  absolute error.\n","\n","To know more about Multi Layer Perceptron (MLP), click [here](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L05%20Multilayer%20Perceptrons.pdf).\n","\n","**Implementation Using Keras**\n","\n","Keras is a high-level Deep Learning API that allows us to easily build train, evaluate and execute all sorts of neural networks. To know more about the documentation of Keras, click [here](https://keras.io/).\n","\n","**Implementation of MLP regression Using sklearn**\n","\n","The very popular machine learning library Scikit-Learn is also capable of basic deep learning modeling.\n","\n","Salient points of Multilayer Perceptron (MLP) in Scikit-learn:\n","\n","* There is no activation function in the output layer.\n","* For regression scenarios, the square error is the loss function, and cross-entropy is the loss function for the classification\n","* It can work with single as well as multiple target values regression.\n","* Unlike other popular packages, likes Keras the implementation of MLP in Scikit doesn’t support GPU.\n","\n","To know more about Scikit-Learn MLP regressor, click [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html).\n"]},{"cell_type":"markdown","metadata":{"id":"mx6BJLwYZBxw"},"source":["### Typical MLP Regressor Architecture"]},{"cell_type":"markdown","metadata":{"id":"GH2a3AvwZNlT"},"source":["\n","Hyperparameter             | Typical Value\n","---------------------------|------------------\n","# input neurons            | One per input feature (e.g., 28 x 28 = 784 for MNIST)\n","# hidden layers            | Depends on the problem. Typically 1 to 5.\n","# neurons per hidden layer | Depends on the problem. Typically 10 to 100.\n","# output neurons           | 1 per prediction dimension\n","Hidden activation          | ReLU\n","Output activation          | None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\n","Loss function              | MSE or MAE/Huber (if outliers)"]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjoZJWGErxGf"},"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"M4_AST_02_MLP_Regression_and_MLP_Tuning_A\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://cds-iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError\n","    else:\n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tAu6v-CnA8zj"},"source":["### Import required packages"]},{"cell_type":"code","metadata":{"id":"7SRN62EfayXM"},"source":["# install livelossplot package to visualize epoch by epoch loss and accuracy curve\n","!pip -qq install livelossplot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fw83tjrgdqNO"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler              # scaling functions from sklearn\n","from sklearn.model_selection import RandomizedSearchCV, train_test_split                  # search on hyperparameters\n","from functools import partial                                                             # partial functions\n","import tensorflow as tf                                                                   # importing tensorflow library\n","from tensorflow import keras                                                              # importing keras package\n","from scipy.stats import reciprocal\n","from sklearn.neural_network import MLPRegressor                                           # importing MLP regressor\n","from tensorflow.keras.optimizers import SGD                                               # stochastic Gradient Descent\n","from tensorflow.keras.utils import to_categorical                                         # converting a class to categorical data type\n","from keras.datasets import mnist                                                          # load mnist dataset\n","import livelossplot                                                                       # visualize loss and accuracy\n","from keras.models import Sequential                                                       # using keras importing Sequential Model\n","from keras.layers import Activation, Dense, Input, Flatten, Dropout, BatchNormalization   # using keras importing layers\n","from keras.callbacks import EarlyStopping                                                 # to stop the training process"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dLVYkKdrdqNV"},"source":["### Building a Regression MLP"]},{"cell_type":"markdown","metadata":{"id":"e5YpbtWMdqNX"},"source":["Here, in this implementation, we will be using California housing problem and tackle it using a regression neural network."]},{"cell_type":"markdown","metadata":{"id":"VE9ZZHrjdqNY"},"source":["#### Data Preparation"]},{"cell_type":"code","metadata":{"id":"j2NZQqhkdqNZ"},"source":["# train dataset\n","df_train = pd.read_csv('/content/sample_data/california_housing_train.csv')\n","df_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-aToiAvNdqNb"},"source":["# test dataset\n","df_test = pd.read_csv('/content/sample_data/california_housing_test.csv')\n","# YOUR CODE HERE to show first five rows of 'df_test'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JyyGRVgVdqNc"},"source":["# printing train dataset information\n","df_train.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qV-lUMrIdqNd"},"source":["# printing test dataset information\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_Vwj6P4dqNe"},"source":["#### Train and Test Data"]},{"cell_type":"code","metadata":{"id":"wISS_1e2dqNf"},"source":["X_train = df_train.drop('median_house_value',axis=1)\n","y_train = df_train['median_house_value']\n","X_test = df_test.drop('median_house_value',axis=1)\n","y_test = df_test['median_house_value']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h9Zhd27edqNg"},"source":["#### Scaling Features"]},{"cell_type":"code","metadata":{"id":"MLnjSOd6dqNh"},"source":["scaler = MinMaxScaler()\n","X_train= scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZDSuABdjdqNi"},"source":["There are two ways to implement MLP regressor one is using keras and the other way is using Scikit-Learn. In this section, we will discuss both two ways."]},{"cell_type":"markdown","metadata":{"id":"8fnY7eWpdqNj"},"source":["#### 1. Using Keras API"]},{"cell_type":"markdown","metadata":{"id":"rzTaNnmudqNk"},"source":["Building, training, evaluating, and using a regression MLP using the Sequential API to make  predictions  is  quite  similar  to  what  we  did  for classification.  The  main  differences  are  the  fact  that  the  output  layer  has  a  single  neuron  (since  we  only  want  to predict  a  single  value)  and  uses  no  activation  function,  and  the  loss  function  is  the mean squared error. Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting:"]},{"cell_type":"code","metadata":{"id":"WdUBydS0dqNl"},"source":["# create a model with two layers\n","model = Sequential([\n","                    Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","                    Dense(1)\n","                    ])\n","model.compile(optimizer='adam', loss='mse')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"glssNoESfQC_"},"source":["Keras supports the early stopping of training via a callback called EarlyStopping.\n","\n","This callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process.\n","\n","The EarlyStopping callback is configured when instantiated via arguments."]},{"cell_type":"code","metadata":{"id":"X7MdewIYdqNm"},"source":["# defining early stop\n","early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n","\n","# fitting the model\n","model.fit(x=X_train,y=y_train.values,\n","          validation_data=(X_test,y_test.values),\n","          batch_size=128,epochs=400, callbacks=[early_stop])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DzYPgOOEdqNn"},"source":["# Sequential Model Summary\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X0IzyLyTdqNo"},"source":["##### Prediction"]},{"cell_type":"code","metadata":{"id":"-_0If9N3dqNp"},"source":["y_pred = model.predict(X_test)\n","# YOUR CODE HERE to show 'y_pred'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"osGmwIw7dqNq"},"source":["##### Evaluation"]},{"cell_type":"code","metadata":{"id":"Eou7_DeldqNr"},"source":["np.sqrt(mean_squared_error(y_test,y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52QacZPvdqNs"},"source":["#### 2. Using Sci-kit Learn API"]},{"cell_type":"markdown","metadata":{"id":"bgmVeddndqNs"},"source":["Using the same dataset, we will implement MLP regressor using sci-kit learn API."]},{"cell_type":"markdown","metadata":{"id":"CUVZ9KE8dqNt"},"source":["In the below code, one hidden layer is modeled with 32 neurons. Considering\n","the input and output layer, we have a total of 5 layers in the model. In case any optimizer is not mentioned then “Adam” is the default optimizer and it can manage a pretty large dataset."]},{"cell_type":"code","metadata":{"id":"EkBzCoekdqNu"},"source":["# implementing MLPregressor\n","regr = MLPRegressor(hidden_layer_sizes=(32), activation=\"relu\", random_state=1, max_iter=500).fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rruum2FKdqNv"},"source":["##### Prediction"]},{"cell_type":"code","metadata":{"id":"5yPBpSl0dqN1"},"source":["y_pred = regr.predict(X_test)\n","# YOUR CODE HERE to show 'y_pred'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l-HUagWGdqN2"},"source":["##### Evaluation"]},{"cell_type":"code","metadata":{"id":"PKmNxwzodqN3"},"source":["np.sqrt(mean_squared_error(y_test,y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehdUQLWSdqN4"},"source":["Now, let us look at the tuning of the neural network hyperparameters or hyperparameter regularization."]},{"cell_type":"markdown","metadata":{"id":"SYgkmzLEdqN5"},"source":["### Fine-Tuning Neural Network Hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"gEYREMR3dqN6"},"source":["The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters  to  tweak.  Not  only  can  we  use  any  imaginable  network  architecture, but even in a simple MLP we can change the number of layers, the number of neurons per layer, the type of activation function to use in each layer, the weight initialization  logic,  and  much  more.\n","\n","One  option  is  to  simply  try  many  combinations  of  hyperparameters  and  see  which one works best on the validation set (or using K-fold cross-validation). For this, one approach  is  simply to use  GridSearchCV  or  RandomizedSearchCV  to  explore  the  hyperparameter space."]},{"cell_type":"markdown","metadata":{"id":"WawLE5TndqN6"},"source":["The first step is to create a function that will build and compile a Keras model, given a set of hyperparameters:"]},{"cell_type":"code","metadata":{"id":"KYsW5K_VdqN7"},"source":["def build_model(hp):\n","  model = keras.Sequential()\n","  model.add(keras.layers.Dense(\n","      hp.Choice('units', [8, 16, 32]),\n","      activation='relu'))\n","  model.add(keras.layers.Dense(1, activation='relu'))\n","  model.compile(loss='mse')\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DBH4CYF5dqN8"},"source":["This function creates a simple Sequential model for univariate regression (only one output  neuron),  with  the  given  input  shape  and  the  given  number  of  hidden  layers and  neurons."]},{"cell_type":"markdown","source":["We want to train hundreds of variants and see which one performs best on the validation set. Since there are many hyperparameters, it is preferable to use a randomized search rather than grid search.\n","\n","Let’s try to explore the number of hidden layers, the number of neurons, and the learning rate:\n","\n","Check out the official documentation of keras tuner [here](https://keras.io/keras_tuner/)"],"metadata":{"id":"THIKT2gz5KB9"}},{"cell_type":"code","metadata":{"id":"JKMjD2XHdqN_"},"source":["param_distribs = {\n","    \"n_hidden\": [0, 1, 2, 3],\n","    \"n_neurons\": np.arange(1, 100),\n","    \"learning_rate\": reciprocal(3e-4, 3e-2),\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tuner = kt.RandomSearch(\n","    build_model,\n","    objective='val_loss',\n","    max_trials=5)"],"metadata":{"id":"TIMF0RoybZ8u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tuner.search(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n","best_model = tuner.get_best_models()[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zep0-WXQfk23","executionInfo":{"status":"ok","timestamp":1700805606442,"user_tz":-330,"elapsed":33785,"user":{"displayName":"CDS Support","userId":"15376517909823222822"}},"outputId":"fdf7d9e6-fef0-4880-8181-c0b0cbebff87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Trial 3 Complete [00h 00m 11s]\n","val_loss: 55104778240.0\n","\n","Best val_loss So Far: 55017631744.0\n","Total elapsed time: 00h 00m 34s\n"]}]},{"cell_type":"markdown","metadata":{"id":"Tqut1OR3dqOE"},"source":["Refer to the guidelines below for choosing the  number  of  hidden  layers  and  neurons  in  an  MLP,  and  selecting  appropriate  values  for some of the main hyperparameters."]},{"cell_type":"markdown","metadata":{"id":"F9t_szGwdqOF"},"source":["#### Number of Hidden Layers"]},{"cell_type":"markdown","metadata":{"id":"EqiYlS6AdqOG"},"source":["- For simple problems, we can start with just one or two hidden layers and get the accurate results.\n","- For more complex problems, we can gradually rampup the number of hidden layers, until we start overfitting the training set. Very complex  tasks,  such  as  large  image  classification  or  speech  recognition,  typically  require networks  with  dozens  of  layers  (or  even  hundreds,  but  not  fully  connected  ones),  and  they  need  a  huge  amount  of  training  data."]},{"cell_type":"markdown","metadata":{"id":"DZu_Fc1ydqOH"},"source":["#### Number of Neurons per Hidden Layer"]},{"cell_type":"markdown","metadata":{"id":"D-1AYlmBdqOI"},"source":["- We can try increasing the number of neurons gradually  until  the  network  starts  overfitting.\n","- In general, it may be more advantageous to increase  the  number  of  layers  than  the  number  of  neurons  per  layer.\n","- A  simpler  approach  is  to  pick  a  model  with  more  layers  and  neurons  than  we actually need, then use early stopping to prevent it from overfitting (and other regularization  techniques,  such  as  dropout, which we will discuss further in this notebook)."]},{"cell_type":"markdown","metadata":{"id":"kQJXWhRIdqOJ"},"source":["#### Learning Rate, Batch Size, and Other Hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"QMuxqHl1dqOJ"},"source":["Here are some of the important hyperparameters other than hidden layers and neurons, and some tips on how to set them:\n","\n","- The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges). So a  simple  approach  for  tuning  the  learning  rate  is  to  start  with  a  large  value  that makes  the  training  algorithm  diverge,  then  divide  this  value  by  3  and  try  again, and repeat until the training algorithm stops diverging.\n","- Choosing  a  better  optimizer  than  plain  old  Mini-batch  Gradient  Descent  (and tuning its hyperparameters) is also quite important. We will discuss this in further sections.\n","- The  batch  size  can  also  have  a  significant  impact  on  our  model’s  performance and the training time. In general the optimal batch size will be lower than 32. We will study batch normalization further in this notebook.\n","- We discussed the choice of the activation function in previous assignment notebook, the $ReLU$ activation function will be a good default for all hidden layers. For the output layer, it really depends on our task.\n","- In  most  cases,  the  number  of  training  iterations  does  not actually  need  to  be tweaked: just use early stopping instead.\n","\n","Let us also take a look at techniques such as Batch normalization, overfitting, drop out, optimizers and learning rate to  train deep neural networks.\n","\n","To know more about hyperparameter tuning of deep neural networks, click [here](https://towardsdatascience.com/the-art-of-hyperparameter-tuning-in-deep-neural-nets-by-example-685cb5429a38)."]},{"cell_type":"markdown","metadata":{"id":"Vc_IEVbFdqOK"},"source":["### Accelerate Learning of Deep Neural Networks With Batch Normalization"]},{"cell_type":"markdown","metadata":{"id":"YRK5Px-qdqOM"},"source":["Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n","\n","![Image](https://lh3.googleusercontent.com/-9hMD_jyPLuE/YNQoKrc-_4I/AAAAAAAACTs/9nZ-BEdtI-QoAe6R5HXlG6X3AcprX8NaQCJEEGAsYHg/s512/2021-06-23.png)\n","\n","$\\text{Figure: Batch Normalization Algorithm}$\n","\n","So  during  training,  BN  just  standardizes  its  inputs  then  rescales  and  offsets  them."]},{"cell_type":"markdown","metadata":{"id":"I_el24cRdqON"},"source":["#### Implementing Batch Normalization with Keras"]},{"cell_type":"markdown","metadata":{"id":"jmjykNm-dqOO"},"source":["Implementing  Batch  Normalization  is  quite  simple, just  add  a  BatchNormalization  layer  before  or  after  each  hidden  layer’s  activation function,  and  optionally  add  a  BN  layer  as  well  as  the  first  layer  in  our  model.  For example,  this  model  applies  BN  after  every  hidden  layer  and  as  the  first  layer  in  the\n","model (after flattening the input images):"]},{"cell_type":"code","metadata":{"id":"HUc-PjdHdqOP"},"source":["# create model with Batch Normalization\n","model = Sequential([\n","                    Flatten(input_shape=[28, 28]),\n","                    BatchNormalization(),\n","                    Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n","                    BatchNormalization(),\n","                    # YOUR CODE HERE to add a Dense layer with 100 neurons with (activation=\"relu\", kernel_initializer=\"he_normal\"),\n","                    # YOUR CODE HERE to add BatchNormalization(),\n","                    # YOUR CODE HERE to add Dense layer with 10 neuron and activation=\"softmax\"\n","                    ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z3u4zONAdqOQ"},"source":["If we display the model summary, we can see that each BN layer adds 4 parameters per input: γ, β, μ and σ (for example, the first BN layer adds 3136 parameters, which is 4 times 784). The last two parameters, μ and σ, are the moving averages, they are not affected by backpropagation, so Keras calls them “Nontrainable” (if we count the total number of BN parameters, 3136 + 1200 + 400, and divide  by  two,  we get  2,368,  which  is  the  total  number  of  non-trainable  params  in this model)."]},{"cell_type":"code","metadata":{"id":"BpyQU5LLdqOR"},"source":["# summary of model\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iaE1qvfhdqOS"},"source":["Let’s look at the parameters of the first BN layer. Two are trainable (by backprop), and two are not:"]},{"cell_type":"code","metadata":{"id":"xTpS00QzdqOS"},"source":["[(var.name, var.trainable) for var in model.layers[1].variables]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h6zEJrikdqOT"},"source":["model.layers[1].updates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Ku_jrTpdqOU"},"source":["Moreover,  since  a  Batch  Normalization layer includes one offset parameter per input, we can remove the bias term from the previous layer (just pass `use_bias=False` when creating it)."]},{"cell_type":"code","metadata":{"id":"lWeHUnCvdqOV"},"source":["# create model\n","model = Sequential([\n","                    # YOUR CODE HERE to add Flatten layer with (input_shape=[28, 28]),\n","                    BatchNormalization(),\n","                    Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n","                    BatchNormalization(),\n","                    Activation(\"relu\"),\n","                    Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n","                    Activation(\"relu\"),\n","                    BatchNormalization(),\n","                    # YOUR CODE HERE to add a Dense layer having 10 neurons with activation=\"softmax\"\n","                    ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-nQq1BuIdqOW"},"source":["The  BatchNormalization class has regularizable hyperparameters. Tweaking the “momentum” argument allows us to control how much of the statistics from the previous mini batch to include when the update is calculated.\n","\n","\n","A good momentum value is typically close to 1, for example, 0.9, 0.99, or 0.999\n","\n","To know more about batch normalization, click [here](https://towardsdatascience.com/batch-normalization-in-practice-an-example-with-keras-and-tensorflow-2-0-b1ec28bde96f)."]},{"cell_type":"markdown","metadata":{"id":"VvBrjDXmdqOX"},"source":["###  Optimizers"]},{"cell_type":"markdown","metadata":{"id":"4Tv15JM0dqOX"},"source":["Some popular optimizers used for boosting the speed in training large deep neural networks are: Momentum optimization, RMSProp, and Adam optimization. Refer [here](https://mlfromscratch.com/optimizers-explained/#/) for a detailed understanding."]},{"cell_type":"markdown","metadata":{"id":"uA4iM1aQdqOY"},"source":["#### Momentum Optimization"]},{"cell_type":"markdown","metadata":{"id":"Yz-Y0M-SdqOZ"},"source":["Momentum  optimization subtracts  the  local  gradient  from  the  momentum  vector  m  (multiplied  by  the  learning  rate  η),  and  it  updates  the  weights  by  simply  adding  this momentum vector, thus accelerating the speed. The momentum hyperparameter $β$ is introduced to prevent  the momentum from growing too large (set between 0 and 1, typically 0.9).\n","\n"]},{"cell_type":"code","metadata":{"id":"wz1B-VDedqOa"},"source":["#Implementing the momentum optimizer\n","optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cFp9_IkfdqOb"},"source":["#### RMSProp"]},{"cell_type":"markdown","metadata":{"id":"1cWLQD2hdqOc"},"source":["The RMSProp algorithm fixes only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step.\n","\n","The decay rate $β$ is typically set to 0.9. Yes, it is once again a new hyperparameter, but this default value often works well, so we may not need to tune it at all.\n"]},{"cell_type":"code","metadata":{"id":"_Ta7gsJFdqOd"},"source":["#Implementing the RMSProp optimizer\n","optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1vPNNs7dqOe"},"source":["#### Adam Optimization"]},{"cell_type":"markdown","metadata":{"id":"wMvwuAridqOf"},"source":["Adam combines the ideas of Momentum  optimization  and  RMSProp:  it keeps track of both, an  exponentially  decaying  average  of  past  gradients,  and  an  exponentially  decaying  average  of  past  squared  gradients.\n","\n","The momentum decay hyperparameter $β_1$ is typically initialized to 0.9, while the scaling  decay  hyperparameter  $β_2$  is  often  initialized  to  0.999."]},{"cell_type":"code","metadata":{"id":"0EfFHwAFdqOg"},"source":["#Implementing the Adam optimizer\n","optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mYIwICgtdqOh"},"source":["To know more about optimizers, click [here](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6#:~:text=Optimizers%20are%20algorithms%20or%20methods,help%20to%20get%20results%20faster)."]},{"cell_type":"markdown","metadata":{"id":"Tt5i_U0udqOh"},"source":["#### Learning Rate Schedule For Training Models"]},{"cell_type":"markdown","metadata":{"id":"I3ln8QNTdqOi"},"source":["The simplest adaptation of learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure."]},{"cell_type":"markdown","metadata":{"id":"hdmHxgkMdqOj"},"source":["##### Time-Based Learning Rate Scheduling"]},{"cell_type":"markdown","metadata":{"id":"PI2I4w5ZdqOk"},"source":["Keras has an in-built time-based learning rate schedule function.\n","\n","The decay argument in the stochastic gradient descent optimization algorithm  is used in the time-based learning rate decay schedule equation as follows:\n","\n","- LearningRate = LearningRate * $\\frac{1}{(1 + decay * epoch)}$\n","\n","When the decay argument is zero (the default), this has no effect on the learning rate.\n","\n","- LearningRate = 0.1 * $\\frac{1}{(1 + 0.0 * 1)} \\implies $LearningRate = 0.1\n","\n","When the decay argument is specified, it will decrease the learning rate from the previous epoch by the given fixed amount.\n","\n","*See the implementation of time-based learning rate scheduling with the MNIST dataset Example at the end of this notebook.*"]},{"cell_type":"markdown","metadata":{"id":"9ltSUwe4dqOu"},"source":["### Regularization"]},{"cell_type":"markdown","metadata":{"id":"pTtpElDodqOv"},"source":["Deep neural networks may have millions of parameters. The network, therefore,   has vast freedom and can fit a huge variety of complex datasets. This flexibility however also makes it prone to overfitting the training set. Thus we need regularization.\n","\n","Let us now see some popular regularization techniques for neural networks: $ℓ1$ and $ℓ2$ regularization and dropout"]},{"cell_type":"markdown","metadata":{"id":"ZD98NJQ6dqOw"},"source":["#### $ℓ1$ and $ℓ2$ Regularization"]},{"cell_type":"markdown","metadata":{"id":"Y7-pZLbsdqOw"},"source":["We can use $ℓ1$ and $ℓ2$ regularization  to  constrain  a  neural  network’s  connection  weights  (but  typically  not  its  biases).  Here  is  how  to  apply  $ℓ2$  regularization  to  a  Keras  layer’s  connection  weights, using a regularization factor of 0.01:"]},{"cell_type":"code","metadata":{"id":"E369mam-dqOx"},"source":["layer = Dense(100, activation=\"relu\",\n","                           kernel_initializer=\"he_normal\",\n","                           kernel_regularizer=keras.regularizers.l2(0.01))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UD9lTIltdqOy"},"source":["Applying the  same  regularizer, activation function and initialization strategy repeatedly to  all  layers  in  our  network may make it error-prone. To avoid this, we can try refactoring our code to use loops. Another option is to use Python’s `functools.partial()` function: it lets us  create  a  thin  wrapper  for  any  callable,  with  some  default  argument  values.  For example:"]},{"cell_type":"code","metadata":{"id":"4IUZfTosdqOz"},"source":["# creating regularized dense layer for model\n","RegularizedDense = partial(keras.layers.Dense,\n","                           activation=\"relu\",\n","                           kernel_initializer=\"he_normal\",\n","                           kernel_regularizer=keras.regularizers.l2(0.01))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pwH4qCwhdqO0"},"source":["# defining model with regularization\n","model = Sequential([\n","    Flatten(input_shape=[28, 28]),\n","    RegularizedDense(300),\n","    # YOUR CODE HERE to add RegularizedDense(100),\n","    RegularizedDense(10, activation=\"softmax\",\n","                     kernel_initializer=\"glorot_uniform\")\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UQ9wGk2FdqO1"},"source":["#### Dropout"]},{"cell_type":"markdown","metadata":{"id":"23QlGnLbdqO2"},"source":["Dropout  is  one  of  the  most  popular  regularization  techniques  for  deep  neural  networks. At each training stage, individual nodes are either dropped out of the net with probability 1-p or kept with probability p, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.\n","\n","![Image](https://i.ibb.co/HnfSTyX/M5-2.jpg)\n","\n","$\\text{Figure: Dropout Regularization}$\n","\n","To  implement  dropout  using  Keras,  we  can  use  the  keras.layers.Dropout  layer. During  training,  it  randomly  drops  some  inputs  (setting  them  to  0)  and  divides  the remaining inputs by the keep probability. After training, it just passes  the  inputs  to  the  next  layer.  For  example,  the  following  code  applies  dropout regularization before every Dense layer, using a dropout rate of 0.2:"]},{"cell_type":"code","metadata":{"id":"_IErGvcSdqO3"},"source":["model = Sequential([\n","                    Flatten(input_shape=[28, 28]),\n","                    Dropout(rate=0.2),\n","                    Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n","                    Dropout(rate=0.2),\n","                    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n","                    # YOUR CODE HERE to add a Dropout layer with (rate=0.2),\n","                    # YOUR CODE HERE to add a Dense layer with 10 neurons and activation=\"softmax\"\n","                    ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BpTgUd2EdqO4"},"source":["If we observe that the model is overfitting, we can increase the dropout rate. Conversely, we should try decreasing the dropout rate if the model underfits the training set."]},{"cell_type":"markdown","metadata":{"id":"9iQC5OcP8cTx"},"source":["Based on the learnings above, let us now explore hyperparameter tuning during the neural network training phase.\n","\n","Here, we implement the sequential model and use the **MNIST dataset**."]},{"cell_type":"markdown","metadata":{"id":"sVb26VZQMLXn"},"source":["#### Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"ZYcT8C3cO7Nc"},"source":["We load the MNIST dataset, using Keras' dataset utilities."]},{"cell_type":"code","metadata":{"id":"JJsnHKAIMsW4"},"source":["# data resizing variables\n","NUM_ROWS = 28\n","NUM_COLS = 28\n","NUM_CLASSES = 10\n","\n","# Load data\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mWHAgEhhVSJE"},"source":["To feed MNIST instances into a neural network, they need to be reshaped, from a 2D image representation to a single dimension sequence. We also convert the class vector to a binary matrix (using to_categorical). This is accomplished below after which the same function defined above is called again in order to show the effects of our data reshaping."]},{"cell_type":"code","metadata":{"id":"t4gQx33YR1ca"},"source":["# Reshape data\n","X_train = X_train.reshape((X_train.shape[0], NUM_ROWS * NUM_COLS))\n","X_train = X_train.astype('float32') / 255\n","X_test = X_test.reshape((X_test.shape[0], NUM_ROWS * NUM_COLS))\n","X_test = X_test.astype('float32') / 255\n","\n","# Categorically encode labels\n","y_train = to_categorical(y_train, NUM_CLASSES)\n","y_test = to_categorical(y_test, NUM_CLASSES)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TBfeJhORQCyB"},"source":["# create the sequential model with BN and dropout layers\n","model = Sequential([\n","    Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n","    # dropout layer to drop neurons with rate less than 0.2\n","    Dropout(rate=0.2),\n","    # BN layer to rescale the inputs\n","    BatchNormalization(),\n","    Activation(\"relu\"),\n","    Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n","    # YOUR CODE HERE to add Dropout layer with (rate=0.2),\n","    Activation(\"relu\"),\n","    # YOUR CODE HERE to add BatchNormalization(),\n","    Dense(10, activation=\"softmax\")\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VgVUeM5yWSTS"},"source":["**Note:** You can also try to define Regularized dense layer and can create a sequential model as we see in the $l1$ and $l2$ regularization section discussed above."]},{"cell_type":"code","metadata":{"id":"rsKGOEeHQg_k"},"source":["# time based learning-rate scheduling\n","epochs = 10\n","learning_rate = 0.1\n","decay_rate = learning_rate / epochs\n","# define optimizer\n","optimizer = keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, decay=decay_rate)\n","\n","# Compile model\n","# YOUR CODE HERE to compile 'model' with (optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RJ3bT-_DUpLq"},"source":["**Note:** In the above code cell, you can also try compiling the with other optimizers like RMS prop, momentum optimization, etc."]},{"cell_type":"code","metadata":{"id":"47V1U9LhQkIJ"},"source":["# outputs epoch-by-epoch loss functions and accuracies at the end of each epoch of training\n","plot_losses = livelossplot.PlotLossesKeras()\n","\n","# Train model\n","model.fit(X_train, y_train,\n","          batch_size=128,\n","          epochs=epochs,\n","          callbacks=[plot_losses],\n","          verbose=1,\n","          validation_data=(X_test, y_test))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPTUDJNjVd_n"},"source":["##### Evaluation"]},{"cell_type":"code","metadata":{"id":"sQhlgALoVgzA"},"source":["score = model.evaluate(X_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u2rywYq6bjG4"},"source":["##### Model Summary"]},{"cell_type":"code","metadata":{"id":"WhEfjsTKbny1"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5v4LSY3AdqO5"},"source":["### Theory Questions"]},{"cell_type":"markdown","metadata":{"id":"Z62bkuLOdqO5"},"source":["**Q1.** How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer?\n","\n","**Answer 1:** Email classification is a binary classification problem, so you would only need one neuron in the output layer. This neuron would indicate the probability that the email is spam or ham. You'd most likely use the sigmoid activation function in the output layer.\n","\n","For the MNIST problem you would need 10 output neurons in the final layer, one for each digit. You would then replace the logistic function with the softmax function which can output one probability per class per digit.\n","\n","**Q2.** Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n","\n","**Answer 2:** In general, the hyperparameters of a neural network you can adjust are the number of hidden layers, the number of neurons in each hidden layer, and the activation function used by each neuron.\n","\n","For binary classification, use the logistic activation function. For a multi-class problem, use softmax. For a linear regression problem, don't use an activation function.\n","\n","Some simple ways to try and solve overfitting are reducing the number of hidden layers or the number of neurons.\n","\n","**Q3.** What  may  happen  if  you  set  the  momentum  hyperparameter  too  close  to  1  (e.g., 0.99999) when using an SGD optimizer?\n","\n","**Answer 3:** If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value.\n","\n","**Q4.** Does dropout slow down training?\n","\n","**Answer 4:** Yes, dropout does slow down training, in general roughly by a factor of two."]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"VgSwVENIPcM6"},"source":["#@title Which of the following is designed to automatically standardize the inputs to a layer in a deep learning neural network? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\", \"Momentum\", \"Batch Normalization\", \"ReLU\", \"Dropout\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}
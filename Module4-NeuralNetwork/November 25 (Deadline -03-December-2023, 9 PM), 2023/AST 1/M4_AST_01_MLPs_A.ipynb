{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gmBawJ8kUNl8"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 1: MLPs"]},{"cell_type":"markdown","metadata":{"id":"o2WYpEkJUNl_"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"MQOkgatUUNl_"},"source":["At the end of the experiment, you will be able to\n","\n","* understand the concept of Perceptron and Multi Layer Perceptron (MLP)\n","* understand the backpropagation algorithm\n","* know different activation functions and gradient issues\n","* build an image classifier using the Keras Sequential API\n","* solve XOR problem with MLP Classification"]},{"cell_type":"markdown","metadata":{"id":"3DS2llW6UNmA"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"E8o5njJtUNmA"},"source":["Artificial Neural Network (ANN) is a Machine Learning model inspired by the networks of biological neurons found in our brains."]},{"cell_type":"markdown","metadata":{"id":"BRPf1QX7UNmB"},"source":["#### Biological Neurons"]},{"cell_type":"markdown","metadata":{"id":"Hw1ifp0HUNmB"},"source":["Biological neurons send and receive signals from the brain. The main component functions of a neuron are - Dendrite: Receives signals from other neurons; Soma: Processes the information; Axon: Transmits the output of this neuron; Synapse: Point of connection to other neurons.\n","\n","<center>\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/10/Blausen_0657_MultipolarNeuron.png\" width= 500 px/>\n","</center>\n","\n","Individual biological neurons are organized in a vast network of billions, with each neuron typically connected to thousands of other neurons. Highly complex computations can be performed by a network of fairly simple neurons.\n"]},{"cell_type":"markdown","metadata":{"id":"V89Kc7-QUNmC"},"source":["#### Artificial Neurons"]},{"cell_type":"markdown","metadata":{"id":"qKdTy3JEUNmC"},"source":["Modeled after human brain activity, artificial neurons are digital constructs that simulate the behavior of biological neurons in some ways. The first computational model of an (artificial) neuron was proposed by Warren McCulloch (neuroscientist) and Walter Pitts (logician) in 1943.\n","\n","As shown below, it may be divided into 2 parts. The first part, g takes an input, performs aggregation, and based on the aggregated value, the second part, f, makes a decision. Understand further through an example 'Watch a football game' in this [article](https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1).\n","\n","<br><br>\n","<center>\n","<img src=\"https://miro.medium.com/max/369/1*fDHlg9iNo0LLK4czQqqO9A.png\" width= 320px/>\n","</center>\n","\n","<br><br>\n"]},{"cell_type":"markdown","metadata":{"id":"ghlEu96VUNmD"},"source":["### The Perceptron"]},{"cell_type":"markdown","metadata":{"id":"KBkkBAD8UNmD"},"source":["The Perceptron is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on a slightly different artificial neuron (shown in the figure below) called a **threshold logic unit (TLU)**. The inputs and the output are numbers (instead of binary on/off values), and each input connection is associated with a weight. The TLU computes a weighted sum of its inputs $$(z = w_1 x_1 + w_2 x_2 + ⋯ + w_n x_n = x^⊺ w)$$, then applies a step function to that sum and outputs the result: $$h_w(x) = step(z)$$, where $z = x^⊺ w$.\n","<br><br>\n","<center>\n","<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1004.png\" width= 400px/>\n","</center>\n","\n","$\\hspace{10cm} \\text {Threshold logic unit}$\n","<br><br>\n","The most common step function used in Perceptrons is the Heaviside step function. Sometimes the sign function is used instead.\n","\n","$$heaviside (z) = \\begin{equation}\n","\\left\\{\n","  \\begin{aligned}\n","    &0&  if\\ \\  z < 0\\\\\n","    &1&  if\\ \\  z \\ge 0\\\\\n","  \\end{aligned}\n","  \\right.\n","\\end{equation}\n","$$\n","\n","$$sgn (z) = \\begin{equation}\n","\\left\\{\n","  \\begin{aligned}\n","    &-1&  if\\ \\  z < 0\\\\\n","    &0&  if\\ \\  z = 0\\\\\n","    &1&  if\\ \\  z > 0\\\\\n","  \\end{aligned}\n","  \\right.\n","\\end{equation}\n","$$\n","\n","A single TLU can be used for simple linear binary classification. It computes a linear combination of the inputs, and if the result exceeds a threshold, it outputs the positive class. Otherwise, it outputs the negative class.\n","\n","\n","\n","The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns (just like Logistic Regression classifiers). However, if the training instances are linearly separable, Rosenblatt demonstrated that this algorithm would converge to a solution. This is called the Perceptron convergence theorem."]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjoZJWGErxGf"},"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"M4_AST_01_MLPs_A\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://cds-iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError\n","    else:\n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Psn-z1duUNmA"},"source":["### Import required packages"]},{"cell_type":"code","metadata":{"id":"ULpxRHv-UNmA"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from sklearn import datasets\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import Perceptron\n","from sklearn.neural_network import MLPClassifier\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Dense\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PgiQmFuzPlxz"},"source":["Scikit-Learn provides a `Perceptron` class that implements a single-TLU network.\n","\n","Let's see how it can be used on the iris dataset:"]},{"cell_type":"code","metadata":{"id":"7d1482YKUNmG"},"source":["# Load Iris dataset\n","iris = datasets.load_iris()\n","# Consider petal length, petal width\n","X = iris.data[:, (2, 3)]\n","# Whether Iris setosa?\n","y = (iris.target == 0).astype(np.int)\n","\n","# Instantiate Perceptron\n","per_clf = Perceptron()\n","# Fit on data\n","# YOUR CODE HERE to fit 'per_clf' on (X, y)\n","\n","# Prediction on one sample instance\n","y_pred = per_clf.predict([[2, 0.5]])\n","# YOUR CODE HERE to show 'y_pred'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tHSOeAjkUNmH"},"source":["Scikit-Learn’s Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no regularization).\n","\n","One of the weaknesses of Perceptrons is that they are incapable of solving some trivial problems (e.g., the Exclusive OR (XOR) classification problem; shown on the left side of the figure given below.\n","\n","But this limitation can be eliminated by stacking multiple Perceptrons. The resulting ANN is called a **Multilayer Perceptron (MLP)**.\n","\n","An MLP can solve the XOR problem, as we can verify by computing the output of the MLP represented on the right side of the figure given below:\n","\n","* with inputs (0, 0) or (1, 1), the network outputs 0, and\n","* with inputs (0, 1) or (1, 0) it outputs 1.\n","\n","All connections have a weight equal to 1, except the four connections where the weight is shown.\n","\n","<center>\n","<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1006.png\" width= 500px/>\n","</center>\n","\n","$\\hspace{8cm} \\text {XOR problem}\\hspace{5 cm} \\text {MLP}$\n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"U8KluZzIUNmI"},"source":["### The Multilayer Perceptron and Backpropagation"]},{"cell_type":"markdown","metadata":{"id":"mZksrPLyUNmI"},"source":["An MLP is composed of\n","\n","* one (passthrough) **input layer**,\n","* one or more layers of TLUs called **hidden layers**, and\n","* one final layer of TLUs called the **output layer** as shown in the figure below.\n","\n","The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers. Every layer except the output layer includes a **bias neuron** and is fully connected to the next layer.\n","\n","<center>\n","<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1007.png\" width= 500px/>\n","</center>\n","\n","When an ANN contains a deep stack of hidden layers, it is called a **deep neural network (DNN)**. The field of Deep Learning studies DNNs, and more generally models containing deep stacks of computations.\n","\n","MLPs are trained using **backpropagation training algorithm**.\n","\n","In short, it is Gradient Descent using an efficient technique for computing the gradients automatically: in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network’s error with regard to every single model parameter.\n","\n","In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.\n","\n","Let’s run through this algorithm in detail:\n","\n","* It handles one mini-batch at a time (say, containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an **epoch**.\n","\n","* Each mini-batch is passed to the network’s **input layer**, which sends it to the first **hidden layer**. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the **output layer**. This is the **forward pass**: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n","\n","* Next, the algorithm measures the network’s output error (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n","\n","* Then it computes how much each output connection contributed to the error.\n","This is done analytically by applying the chain rule, which makes this step fast and precise.\n","\n","* The algorithm then measures how much of these error contributions came from\n","each connection in the layer below, again using the chain rule, working backward\n","until the algorithm reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network.\n","\n","* Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n","\n","Let's summarize this algorithm again: for each training instance, the backpropagation algorithm first makes a prediction (**forward pass**) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (**reverse pass**), and finally tweaks the connection weights to reduce the error (Gradient Descent step).\n","\n","In order for this algorithm to work properly, the step function was replaced with an activation function."]},{"cell_type":"markdown","metadata":{"id":"rU4U0ZONUNmI"},"source":["### Activation Functions"]},{"cell_type":"markdown","metadata":{"id":"oiC5tHMRUNmI"},"source":["Replacing the step function with the activation function was essential because the step function contains only flat segments, so there is no gradient to work with (Gradient Descent cannot move on a flat surface), while the activation function has a well-defined nonzero derivative, allowing Gradient Descent to make some progress at every step.\n","\n","Some of the activation functions are shown below:\n","\n","* **Logistic (sigmoid) function:**\n","\n","$$σ(z) = \\frac{1}{1 + exp(–z)}$$\n","\n","It is an S-shaped function, exists between $0$ to $1$. Therefore, it is especially used for models where we have to predict the probability as an output. The function is differentiable.\n","That means, we can find the slope of the sigmoid curve at any two points.\n","\n","* **Hyperbolic tangent function:**\n","\n","$$tanh(z) = 2σ(2z) – 1 = \\frac{2}{1 + exp(–2z)} - 1$$\n","\n","Just like the logistic function, this activation function is S-shaped, continuous, and differentiable, but its output value ranges from $–1$ to $1$. That range tends to make each layer’s output more or less centered around $0$ at the beginning of training, which often helps speed up convergence.\n","\n","* **Rectified Linear Unit function:**\n","\n","$$ReLU(z) = max(0, z)$$\n","\n","The ReLU function is continuous but unfortunately not differentiable at $z = 0$\n","(the slope changes abruptly, which can make Gradient Descent bounce around),\n","and its derivative is $0$ for $z < 0$. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default. Most importantly, the fact that it does not have a maximum output value helps reduce some issues during Gradient Descent.\n","\n","These popular activation functions and their derivatives are represented in\n","the figure below.\n","<br><br>\n","<center>\n","<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1008.png\" width=700px/>\n","</center>\n","<br><br>\n","\n","**Why do we need activation functions?**\n","\n","If we chain several linear transformations, all we get is a linear transformation. For example, if $f(x) = 2x + 3$ and $g(x) = 5x – 1$, then chaining these two linear functions gives you another linear function: $f(g(x)) = 2(5x – 1) + 3 = 10x + 1.$\n","\n","So if we don’t have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, and we can’t solve very complex problems with that. Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function.\n","\n","Training a DNN is not an easy task. Let's see what difficulties we may encounter during that process."]},{"cell_type":"markdown","metadata":{"id":"eChoS2HgUNmI"},"source":["### Gradient Issues"]},{"cell_type":"markdown","metadata":{"id":"Cu8bAUaaUNmJ"},"source":["Some of the problems we could run into while training a deep DNN are:\n","\n","* **vanishing gradients** when the gradients grow smaller and smaller,\n","\n","* **exploding gradients** when the gradients grow larger and larger,\n","\n","when flowing backward through the DNN during training. Both of these problems make lower layers very hard to train.\n","\n","Once the backpropagation algorithm has computed the gradient of the cost function with regard to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.\n","\n","Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution. We call this the **vanishing gradients** problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is the **exploding gradients** problem, which surfaces in recurrent neural networks. More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n","\n","In 2010 it was discovered the problem was with the popular logistic sigmoid activation function and the weight initialization technique (i.e., a normal distribution with a mean of $0$ and a standard deviation of $1$).\n","\n","Looking at the logistic activation function shown in the figure below, we can see that when inputs become large (negative or positive), the function saturates at $0$ or $1$, with a derivative extremely close to $0$. Thus, when backpropagation comes in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is nothing left for the lower layers.\n","<br><br>\n","<center>\n","<img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/gradients-in-sigmoid-activation-functions.jpg\" width= 450px/>\n","</center>\n"]},{"cell_type":"markdown","metadata":{"id":"vpqSnuwNUNmJ"},"source":["In their paper, Glorot and Bengio proposed: the connection weights of each layer must be initialized randomly to significantly alleviate the unstable gradients problem. This initialization strategy is called **Xavier initialization** or **Glorot initialization**, after the paper’s first author.\n","\n","Glorot initialization (when using the logistic activation function):\n","\n","Normal distribution with mean 0 and variance $σ^2 = \\frac{1}{fan_{avg}}$\n","\n","Or a uniform distribution between $−r$ and $+r$, with $r = \\sqrt{\\frac{3}{fan_{avg}}}$\n","\n","where $fan_{avg} = \\frac{fan_{in} + fan_{out}}{2}$,\n","\n","$fan_{in}$ and $fan_{out}$ are the number of inputs and neurons of the layer.\n","\n","Similar strategies for different activation functions differ only by the scale of the variance and whether they use $fan_{avg}$ or $fan_{in}$, as shown in the table below (for the uniform distribution, just compute $r = \\sqrt{3σ^2}$).\n","\n","The initialization strategy for the ReLU activation function and its variants is sometimes called **He initialization**.\n","\n","| Initialization | Activation functions          | $\\sigma ^2$ Normal |\n","|:---------------|:------------------------------|:-------------------|\n","| Glorot         | None, tanh, logistic, softmax | $1$/$fan_{avg}$    |\n","| He             | ReLU and variants             | $2$/$fan_{avg}$    |\n","\n","By default, Keras uses Glorot initialization with a uniform distribution. When creating a layer, we can change this to He initialization by setting `kernel_initializer=\"he_uniform\"` or `kernel_initializer=\"he_normal\"`.\n"]},{"cell_type":"markdown","metadata":{"id":"B_ey0gh9UNmJ"},"source":["### MLP Classifiers"]},{"cell_type":"markdown","metadata":{"id":"Uwx1h5H2UNmJ"},"source":["MLPs can be used for classification and regression tasks. In classification, they can perform (i) Binary Classification (ii) Multilabel Binary Classification, and (iii) Multiclass classification\n","\n","* **Binary classification:** Used when there are only two distinct classes and the data we want to classify belongs exclusively to one of those classes, e.g. classifying if a review sentiment is positive or negative.\n","\n","* **Multilabel binary classification:** Used when there are two or more classes and the data we want to classify belongs to none of the classes or all of them at the same time, e.g. classifying which traffic signs are shown in an image.\n","\n","  Note that the output probabilities do not necessarily add up to 1. This lets the model output any combination of labels\n","\n","* **Multiclass classification:** Used when there are three or more classes and the data we want to classify belongs exclusively to one of those classes, e.g.  out of three or more possible classes (e.g., classes 0 through 9 for digit image classification), we need to have one output neuron per class, and we should use the **softmax activation function** for the whole output layer as shown in the figure below. The softmax function will ensure that all the estimated probabilities are between $0$ and $1$ and that they add up to $1$.\n","<br><br>\n","<center>\n","<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1009.png\" width=500px/>\n","</center>\n","\n","Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (also called the log loss) is generally a good choice."]},{"cell_type":"markdown","metadata":{"id":"kMPHSAZkUNmJ"},"source":["### Building an Image Classifier Using the Sequential API"]},{"cell_type":"markdown","metadata":{"id":"Wa4NA08wUNmK"},"source":["Here we consider the Fashion MNIST dataset. It has the same format as MNIST (70,000 grayscale images of 28 × 28 pixels each, with 10 classes), but the images represent fashion items rather than handwritten digits, so each class is more diverse, and the problem turns out to be significantly more challenging than MNIST."]},{"cell_type":"code","metadata":{"id":"ARPWU1DpUNmK"},"source":["# Using Keras to load the dataset\n","fashion_mnist = keras.datasets.fashion_mnist\n","(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJQAox8qUNmK"},"source":["# Shape and datatype of X_train_full\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hYUIpYB8UNmK"},"source":["Note that the dataset is already split into a training set and a test set, but there is no validation set, so we’ll create one now. Additionally, since we are going to train the neural network using Gradient Descent, we must scale the input features. For simplicity, we’ll scale the pixel intensities down to the 0–1 range by dividing them by 255.0:"]},{"cell_type":"code","metadata":{"id":"-PWLthKwUNmK"},"source":["# Validation set and scaling\n","X_valid = X_train_full[:5000] / 255.0\n","X_train = X_train_full[5000:] / 255.0\n","y_valid = y_train_full[:5000]\n","y_train = y_train_full[5000:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sBtXrFZAUNmK"},"source":["With MNIST, when the label is equal to 5, it means that the image represents the\n","handwritten digit 5. For Fashion MNIST, however, we need the list of class\n","names to know what we are dealing with:"]},{"cell_type":"code","metadata":{"id":"da1cMIQhUNmL"},"source":["# List of labels\n","class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n","\n","# First image in the training set\n","class_names[y_train[0]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WRd-gAW2UNmL"},"source":["**Creating the model using the Sequential API**\n","\n","Here is a classification MLP with two hidden layers:"]},{"cell_type":"code","metadata":{"id":"7PN3orqbUNmL"},"source":["# Create model with 2 hidden layers and one output layer\n","model = Sequential()\n","model.add(Flatten(input_shape=[28, 28]))\n","model.add(Dense(300, activation=\"relu\"))\n","# YOUR CODE HERE to add a Dense layer with 100 neurons and activation=\"relu\"\n","# YOUR CODE HERE to add a Dense layer with 10 neurons and activation=\"softmax\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"caNaKaqWUNmL"},"source":["Let’s go through the above code line by line:\n","\n","* The first line creates a Sequential model. This is the simplest kind of Keras\n","model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the Sequential API.\n","\n","* Next, we build the first layer and add it to the model. It is a Flatten layer whose role is to convert each input image into a 1D array: if it receives input data X, it computes X.reshape(-1, 1). This layer does not have any parameters; it is just there to do some simple preprocessing. Since it is the first layer in the model, we should specify the input_shape, which doesn’t include the batch size, only the shape of the instances. Alternatively, we could add a `keras.layers.InputLayer` as the first layer, setting input_shape=[28,28].\n","\n","* Next, we add a Dense hidden layer with 300 neurons. It will use the ReLU activation function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes $h_{W, b}(X) = ϕ( XW + b)$.\n","\n","* Then we add a second Dense hidden layer with 100 neurons, also using the ReLU\n","activation function.\n","\n","* Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive).\n","\n","Instead of adding the layers one by one we can pass a list of layers when creating the Sequential model:\n"]},{"cell_type":"code","metadata":{"id":"nL6fK7WiUNmL"},"source":["# Create model with 2 hidden layers and one output layer\n","model = Sequential([\n","                    Flatten(input_shape=[28, 28]),\n","                    Dense(300, activation=\"relu\"),\n","                    # YOUR CODE HERE to add a Dense layer with 100 neurons and activation=\"relu\"\n","                    Dense(10, activation=\"softmax\")\n","                    ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tTPvvL1cUNmL"},"source":["The model’s `summary()` method displays all the model’s layers, including each layer’s name, its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters. Here we only have trainable parameters."]},{"cell_type":"code","metadata":{"id":"reKEBMErUNmL"},"source":["# Summary of model\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x4wlf6WLUNmM"},"source":["**Compiling the model**\n","\n","After a model is created, we must call its `compile()` method to specify the loss function and the optimizer to use."]},{"cell_type":"code","metadata":{"id":"NUObYWaCUNmM"},"source":["# Compile model\n","model.compile(loss=\"sparse_categorical_crossentropy\",\n","              optimizer=\"sgd\",\n","              metrics=[\"accuracy\"]\n","              )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xqho50J7UNmM"},"source":["In the above code cell,\n","\n","* first, we use the **\"sparse_categorical_crossentropy\"** loss because we have sparse labels (i.e., for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive. If instead, we had one target probability per class for each instance (such as one-hot vectors, e.g. [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would need to use the \"categorical_crossentropy\" loss instead. If we were doing binary classification (with one or more binary labels), then we would use the \"sigmoid\" activation function in the output layer instead of the \"softmax\" activation function, and we would use the \"binary_crossentropy\" loss.\n","\n","* Regarding the optimizer, **\"sgd\"** means that we will train the model using simple Stochastic Gradient Descent.\n","\n","* Finally, since this is a classifier, it’s useful to measure its **\"accuracy\"** during training and evaluation."]},{"cell_type":"markdown","metadata":{"id":"r5w_4VMhUNmM"},"source":["**Training and evaluating the model**\n"]},{"cell_type":"code","metadata":{"id":"PDaejclVUNmM"},"source":["# Training model on Training set\n","history = model.fit(X_train, y_train, epochs=30, validation_data = (X_valid, y_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_-VeynWUNmM"},"source":["The `fit()` method returns a History object containing the training parameters\n","(`history.params`), the list of epochs it went through (`history.epoch`), and most importantly a dictionary (`history.history`) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any).\n","\n","Let's plot the learning curves:"]},{"cell_type":"code","metadata":{"id":"RI4BnS5xUNmN"},"source":["# Visualize training and validation metrics\n","df = pd.DataFrame(history.history)\n","df.plot(figsize=(8, 5))\n","plt.grid(True)\n","# set the vertical range to [0-1]\n","plt.gca().set_ylim(0, 1)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fnnR_Ow7UNmN"},"source":["We can see that both the training accuracy and the validation accuracy steadily\n","increase during training, while the training loss and the validation loss decrease. Moreover, the validation curves are close to the training curves, which means that there is not too much overfitting.\n","\n","Once we are satisfied with the model’s validation accuracy, we should evaluate it on the test set to estimate the generalization error before we deploy the model to production. We can easily do this using the `evaluate()` method:"]},{"cell_type":"code","metadata":{"id":"QaJZC6zPUNmN"},"source":["# Model performance on test set\n","# YOUR CODE HERE using 'model' to evaluate(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DWA6HUC1UNmO"},"source":["**Using the model to make predictions**\n","\n","We can use the model’s `predict()` method to make predictions on new instances."]},{"cell_type":"code","metadata":{"id":"-QYxeL6iUNmO"},"source":["# Predict class probabilities for first three instances of X_test\n","X_new = X_test[:3]\n","# YOUR CODE HERE to create 'y_proba' using 'model' to predict(X_new)\n","y_proba.round(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IRVjwIzeUNmO"},"source":["# Predict class labels for first three instances of X_test\n","y_pred = np.argmax(model.predict(X_new), axis=-1)\n","# YOUR CODE HERE to show 'y_pred'\n","\n","print(np.array(class_names)[y_pred])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRoOhS4AUNmO"},"source":["# Actual labels\n","y_new = y_test[:3]\n","# YOUR CODE HERE to show 'y_new'\n","\n","fig, ax = plt.subplots(1,3)\n","for axi, i in zip(ax.ravel(), np.arange(len(X_new))):\n","    axi.imshow(X_new[i], cmap='Greys')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PlbKDTz2UNmP"},"source":["From the above results, we can see that the classifier actually classified all three images correctly.\n","\n","Let's see one example with the XOR dataset."]},{"cell_type":"markdown","metadata":{"id":"-SeYqgf5UNmP"},"source":["**Exercise 1:** Generate the XOR dataset and perform classification using the MLPClassifier."]},{"cell_type":"code","metadata":{"id":"AbMaHF9BUNmP"},"source":["# generate 50 random numbers for 4 quadrants represents XOR\n","x1 = np.random.uniform(1,3,50)\n","x2 = np.random.uniform(4,6,50)\n","y1 = np.random.uniform(1,3,50)\n","y2 = np.random.uniform(4,6,50)\n","\n","# features\n","X_1 = np.vstack([np.append(x1,x2), np.append(y1,y2)]).T\n","X_2 = np.vstack([np.append(x1,x2), np.append(y2,y1)]).T\n","X = np.vstack([X_1, X_2])\n","\n","# label 0 and 1\n","y_1 = [0 for i in range(len(X_1))]\n","y_2 = [1 for i in range(len(X_2))]\n","y = np.append(y_1, y_2)\n","\n","# stack features and labels\n","data = np.hstack([X,y.reshape(-1,1)])\n","\n","# shuffle the dataset\n","np.random.shuffle(data)\n","# Split the data\n","X, y = data[:,:2], data[:,2]\n","X = StandardScaler().fit_transform(X)\n","\n","# Visualize data\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zWBDc394UNmP"},"source":["# Train a single Perceptron\n","per_clf = Perceptron()\n","# YOUR CODE HERE to fit 'per_clf' on (X, y)\n","pred = per_clf.predict(X)\n","accuracy_score(y, pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IlomLn-nUNmP"},"source":["# Visualize prediction using single Perceptron\n","plt.scatter(X[:, 0], X[:, 1], c=pred, s=50, cmap='autumn')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PrETiJonUNmP"},"source":["From the above results, we can see that a single perceptron is unable to classify two classes. Let's use a multilayer perceptron with single hidden layer having 4 neurons:"]},{"cell_type":"code","metadata":{"id":"M8xDyEbiUNmP"},"source":["# Train an MLP classifier\n","model = MLPClassifier(activation='relu',\n","                      max_iter=10000,\n","                      hidden_layer_sizes=(4)\n","                      )\n","model.fit(X, y)\n","# YOUR CODE HERE using 'model' to predict(X)\n","# YOUR CODE HERE to show accuracy_score of (y, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tcaOhc0jUNmQ"},"source":["# Visualize prediction using multilayer perceptron\n","# YOUR CODE HERE for scatter plot (X[:, 0], X[:, 1]) with parameters (c=y_pred, s=50, cmap='autumn')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QvYgUYhWUNmQ"},"source":["From the above results, we can see that MLP classifies the two classes almost accurately."]},{"cell_type":"markdown","metadata":{"id":"MBMqT7KgPlyB"},"source":["### Theory Questions"]},{"cell_type":"markdown","metadata":{"id":"zyjO7cGKPlyB"},"source":["1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of threshold logic units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n","\n"," A classical Perceptron will converge only if the dataset is linearly separable, and it won’t be able to estimate class probabilities. In contrast, a Logistic Regression classifier will converge to a good solution even if the dataset is not linearly separable, and it will output class probabilities. If you change the Perceptron’s activation function to the logistic activation function (or the softmax activation function if there are multiple neurons), and if you train it using Gradient Descent (or some other optimization algorithm minimizing the cost function, typically cross entropy), then it becomes equivalent to a Logistic Regression classifier.\n","\n","2. Why was the logistic activation function a key ingredient in training the first MLPs?\n","\n"," The logistic activation function was a key ingredient in training the first MLPs because its derivative is always nonzero, so Gradient Descent can always roll down the slope. When the activation function is a step function, Gradient\n","Descent cannot move, as there is no slope at all.\n","\n","3. Name three popular activation functions.\n","\n"," Popular activation functions include the step function, the logistic (sigmoid)\n","function, the hyperbolic tangent (tanh) function, and the Rectified Linear Unit\n","(ReLU) function.\n","\n","4. Suppose you have an MLP composed of one input layer with 10 passthrough\n","neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n","\n","  * What is the shape of the input matrix $X$?\n","  * What are the shapes of the hidden layer’s weight vector $W_h$ and its bias vector $b_h$?\n","  * What are the shapes of the output layer’s weight vector $W_o$ and its bias vector $b_o$?\n","  * What is the shape of the network’s output matrix $Y$?\n","  * Write the equation that computes the network’s output matrix $Y$ as a function of $X, W_h, b_h, W_o,$ and $b_o$.\n","\n"," Considering the MLP described in the question, composed of one input layer\n","with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons, where all artificial neurons use the ReLU activation function:\n","\n","  * The shape of the input matrix X is $m$ × 10, where $m$ represents the training batch size.\n","\n","  * The shape of the hidden layer’s weight vector $W_h$ is 10 × 50, and the length of its bias vector $b_h$ is 50.\n","\n","  * The shape of the output layer’s weight vector $W_o$ is 50 × 3, and the length of its bias vector $b_o$ is 3.\n","\n","  * The shape of the network’s output matrix $Y$ is $m$ × 3.\n","\n","  * $Y = ReLU(ReLU(X W_h + b_h) W_o + b_o)$.\n","  \n","  Recall that the ReLU function just sets every negative number in the matrix to zero. Also note that when you are\n","adding a bias vector to a matrix, it is added to every single row in the matrix,\n","which is called broadcasting.\n","\n","5. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, and which activation function should you use? What about for getting your network to predict housing prices?\n","\n","  To classify email into spam or ham, you just need one neuron in the output layer of a neural network—for example, indicating the probability that the email is spam. You would typically use the logistic activation function in the output layer when estimating a probability. If instead you want to tackle MNIST, you need 10 neurons in the output layer, and you must replace the logistic function with the softmax activation function, which can handle multiple classes, outputting one probability per class. If you want your neural network to predict housing prices, then you need one output neuron, using no activation function at all in the output layer.\n","\n","6. What is backpropagation and how does it work? What is the difference between\n","backpropagation and reverse-mode autodiff?\n","\n","  Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regard to every model parameter (all the weights and biases), then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thousands or millions of times, using many training batches, until the model parameters converge to values that (hopefully) minimize the cost function. To compute the gradients, backpropagation uses reverse-mode autodiff (although it wasn’t called that when backpropagation was invented, and it has been reinvented several times). Reverse-mode autodiff performs a forward pass through a computation graph, computing every node’s value for the current training batch, and then it performs a reverse pass, computing all the gradients at once.\n","  \n","  So the difference is, backpropagation refers to the whole process of training an artificial neural network using multiple backpropagation steps, each of which computes gradients and uses them to perform a Gradient Descent step. In contrast, reverse-mode autodiff is just a technique to compute gradients efficiently, and it happens to be used by backpropagation."]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"VgSwVENIPcM6"},"source":["#@title Suppose the network has 784 inputs, 32 neurons in first hidden layer, 16 neurons in second hidden layer, and 10 neurons in the output layer. How many parameters does the network have? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\", \"25818\", \"25760\", \"842\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}